<!DOCTYPE html> <html> <head> <meta charset='utf-8'> <meta http-equiv='X-UA-Compatible' content='IE=edge'> <meta name='viewport' content='width=device-width, initial-scale=1'> <meta name='description' content='CSC2541 course website'> <title>CSC2541 Scalable and Flexible Models of Uncertainty (Fall 2017)</title> <link rel="shortcut icon" type="image/png" href='/assets/favicons.png'> <link href='/assets/app.css' rel='stylesheet' type='text/css'> <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries --> <!-- WARNING: Respond.js doesn't work if you view the page via file:// --> <!--[if lt IE 9]> <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script> <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script> <![endif]--> </head> <body> <nav role='navigation'> <div class='container'> <div class='navbar-header'> <button type='button' data-toggle='collapse' data-target='#menu'> <span class='sr-only'>Toggle navigation</span> <span class='icon-bar'></span> <span class='icon-bar'></span> <span class='icon-bar'></span> </button> <a class='navbar-brand' href='#'></a> </div> <div id='menu'> <ul> <li> <a href='#/contents/01-info'>Course Information</a> </li> <li> <a href='#/contents/02-calendar'>Calendar</a> </li> <li> <a href='#/contents/03-software'>Software</a> </li> </ul> </div> </div> </nav> <header> <div class='headline'> <div class='container'> <h3>University of Toronto, Fall 2017</h3> <h3>CSC2541&#58; Scalable and Flexible Models of Uncertainty</h3> </div> </div> </header> <div class="container"> <main> <hr id='/contents/01-info'> <section> <h2><span>Course Information&#58;</span></h2> <div class='lead'><p><strong>Instructor:</strong> <a href="http://www.cs.toronto.edu/~rgrosse">Roger Grosse</a></p> <p><strong>Email:</strong> rgrosse at cs dot toronto dot edu (put CSC2541 in the subject line)</p> <p><strong>Lecture:</strong> Friday, 2-4pm, in Bahen 1200</p> <p><strong>Office Hours:</strong></p> <ul> <li> <p>Tuesday, 1-3pm, in Pratt 290F (that’s the D. L. Pratt Building, not the E. J. Pratt Library!)</p> </li> <li> <p>You need to book a time slot through a URL which will be given out during class.</p> </li> <li> <p>Your team should book a double time slot the week of your presentation. Please bring a draft of your presentation.</p> </li> </ul> <h3 id="overview">Overview</h3> <p>Over the last 5 years or so, neural networks have driven rapid progress in applications as diverse as vision, language understanding, speech understanding, robotics, chemistry, and game playing. But a major challenge remains: modeling uncertainty, i.e. knowing what one doesn’t know. Good models of uncertainty are crucial whenever an algorithm needs to manage exploration, i.e. decide how and when to acquire new information. The topic of uncertainty also rears its head in the context of adversarial examples, a recently discovered phenomenon which originally seemed like a curiosity, but now seems to be a serious security vulnerability for modern ML systems which has so far resisted all attempts to defeat it.</p> <p>The first half of the course will cover a set of algorithmic tools for modeling uncertainty: Gaussian processes, Bayesian neural nets, and variational inference. We will focus on continuous models and the setting of function approximation, in order to avoid overlap with other iterations of this course (see below). I.e., we will have little if any coverage of generative models or discrete latent variables. The second half of the course will cover applications of uncertainty modeling: neural net sparsification, active learning, black-box optimization, reinforcement learning, and adversarial robustness.</p> <h3 id="prerequisites">Prerequisites</h3> <p>This course is designed to bring students to the current frontier of knowledge on these methods, so that ideally, their course projects can make a novel contribution. A previous background in machine learning such as CSC411 or ECE521 is strongly recommended. Linear algebra, basic multivariate calculus, basics of working with probability, and programming skills are required.</p> <h3 id="relationship-with-other-courses">Relationship with other courses</h3> <p>Since uncertainty is a central topic in machine learning, it’s unsurprising that there are lots of courses at UofT focusing on it. Several departments offer core courses focused on probabilistic machine learning, with varying emphases:</p> <ul> <li><a href="https://www.cs.toronto.edu/~duvenaud/courses/csc412/index.html">CSC412, Probabilistic Learning and Reasoning</a></li> <li><a href="https://duvenaud.github.io/sta414/">STA414, Statistical Methods for Machine Learning</a></li> <li><a href="https://ece521.github.io/">ECE521, Inference Algorithms and Machine Learning</a></li> </ul> <p>Those courses are all lecture-based, and aim to give broad coverage of probabilistic modeling techniques. They focus on principles which are pretty well understood.</p> <p>By contrast, CSC2541 is a topics course which aims to bring you to the research frontier on certain topics. Many of the topics have not yet been distilled in an easily accessible form, and a lot of the key experimental findings are still open to multiple interpretations. While the core courses listed above cover a variety of techniques, here we focus on a smaller set (mostly Gaussian processes, Bayesian neural nets, and variational inference) and look at how they can be applied in situations that depend on accurate uncertainty modeling.</p> <p>CSC2541 is a topics course which is offered repeatedly but with different topics. I’m only planning to offer this version of the course once. David Duvenaud recently taught a <a href="https://www.cs.toronto.edu/~duvenaud/courses/csc2541/index.html">version of 2541</a> focused on generative models, and this coming winter, he’ll be teaching a topics course focused on learning discrete structure. I’ve tried to minimize the overlap with those courses, so if you’ve taken 2541 before, you should be able to take it again without being bored. Hence, this course doesn’t have much material on generative models or discrete latent variables. I think there are roughly 2 weeks of overlap with last year’s iteration of 2541: variational inference and model-based reinforcement learning.</p> <h3 id="course-structure">Course structure</h3> <p>After the first two lectures, each week a different team of students will present on the readings for that week. I’ll provide guidance about the content of these presentations.</p> <p>In-class discussion will center around:</p> <ul> <li>Understanding the strengths and weaknesses of these methods.</li> <li>Understanding the relationships between these methods, and with previous approaches.</li> <li>Extensions or applications of these methods.</li> <li>Experiments that might better illuminate their properties.</li> </ul> <p>The hope is that these discussions will lead to actual research papers, or resources that will help others understand these approaches.</p> <p>Grades will be based on:</p> <ul> <li>Class presentations - 20%</li> <li>Project proposal - 20% - due Oct. 12</li> <li>Final project presentation, report, and code - 60% <ul> <li>presentations Nov. 24 and Dec. 1</li> <li>Project report due Dec. 10</li> </ul> </li> </ul> <h3 id="project">Project</h3> <p>You are asked to carry out an original research project related to the course content and write a (roughly 8 page) report. You’re encouraged to work in teams of 3-5. See <a href="project-handout.pdf">here</a> for more specific guidelines.</p> </div> </section> <hr id='/contents/02-calendar'> <section> <h2><span>Calendar&#58;</span></h2> <div class='lead'><table> <tr> <th> </th> <th> Topic </th> <th> Readings </th> </tr> <tr> <td> 9/15 </td> <td> Overview </td> <td> Overview <ul> <li><a href="http://www.nature.com/nature/journal/v521/n7553/full/nature14541.html">Ghahramani, 2015. Probabilistic machine learning and artificial intelligence.</a></li> </ul> Bayesian regression <ul> <li><a href="http://authors.library.caltech.edu/13792/1/MACnc92a.pdf">MacKay, 1992. Bayesian interpolation.</a></li> <li><a href="http://mlg.eng.cam.ac.uk/zoubin/papers/occam.pdf">Rasmussen and Ghahramani, 2001. Occam's razor.</a> </li> <li>Review: <a href="https://metacademy.org/graphs/concepts/bayesian_parameter_estimation">Bayesian parameter estimation</a>, <a href="https://metacademy.org/graphs/concepts/bayesian_linear_regression">Bayesian linear regression</a></li> </ul> Calibration <ul> <li><a href="https://arxiv.org/abs/1706.04599">Guo et al., 2017. On calibration of modern neural networks.</a></li> </ul> </td> </tr> <tr> <td> 9/22 </td> <td> Gaussian Processes </td> <td> Foundations <ul> <li><a href="http://www.gaussianprocess.org/gpml/chapters/">GPML, Chapter 2</a></li> </ul> Structured kernels <ul> <li><a href="http://www.gaussianprocess.org/gpml/chapters/">skim GPML sections 4.1-4.2</a></li> <li><a href="http://www.gaussianprocess.org/gpml/chapters/">GPML, Chapter 5</a></li> <li><a href="https://www.cs.toronto.edu/~duvenaud/cookbook/index.html">David Duvenaud's kernel cookbook</a></li> <li><a href="http://proceedings.mlr.press/v28/wilson13.pdf">Wilson and Adams, 2013. Gaussian process kernels for pattern discovery and extrapolation</a></li> <li><a href="http://proceedings.mlr.press/v28/duvenaud13.pdf">Duvenaud et al., 2013. Structure discovery in nonparametric regression through compositional kernel search</a></li> </ul> </td> </tr> <tr> <td> 9/29 </td> <td> Bayesian Neural Nets </td> <td> Background <ul> <li><a href="https://metacademy.org/graphs/concepts/backpropagation">backpropagation</a></li> <li><a href="https://metacademy.org/graphs/concepts/metropolis_hastings">Metropolis-Hastings</a></li> </ul> Foundations <ul> <li><a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.3.448">MacKay, 1992. A practical Bayesian framework for backpropagation networks.</a></li> <li><a href="http://www.csri.utoronto.ca/~radford/ftp/thesis.pdf">Neal, 1995. Bayesian learning for neural networks.</a> Chapter 2.</li> </ul> Hamiltonian Monte Carlo <ul> <li><a href="https://arxiv.org/abs/1206.1901">Neal, 2012. MCMC using Hamiltonian dynamics.</a> (focus on sections 1-3)</li> </ul> Stochastic gradient Langevin dynamics <ul> <li><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Welling_398.pdf">Welling and Teh, 2011. Bayesian learning via stochastic gradient Langevin dynamics.</a></li> <li><a href="http://papers.nips.cc/paper/5965-bayesian-dark-knowledge">Balan et al., 2015. Bayesian dark knowledge</a></li> </ul> </td> </tr> <tr> <td> 10/6 </td> <td> Variational Inference for BNNs </td> <td> Background <ul> <li><a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">variational Bayes</a></li> </ul> Variational inference for BNNs <ul> <li><a href="http://dl.acm.org/citation.cfm?id=168306">Hinton and van Camp, 1993. Keeping the neural networks simple by minimizing the description length of the weights.</a></li> <li><a href="http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks">Graves, 2011. Practical variational inference for neural networks</a></li> <li><a href="http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick">Kingma, Salimans, and Welling, 2015. Variational dropout and the local reparameterization trick.</a></li> <li><a href="http://proceedings.mlr.press/v48/gal16.html">Gal and Ghahramani, 2016. Dropout as Bayesian approximation: representing model uncertainty in deep learning</a></li> </ul> Sparsification <ul> <li><a href="https://arxiv.org/abs/1705.08665">Louizos et al., 2017. Bayesian compression for deep learning.</a></li> </ul> </td> </tr> <tr> <td> 10/12 </td> <td> Project proposals due! </td> <td> Send by e-mail to csc2541-submit at cs dot toronto dot edu. Include "CSC2541 Project Proposal" in subject line. </td> </tr> <tr> <td> 10/13 </td> <td> Variational Inference for GPs </td> <td> <p>Note: this is among the most mathematically demanding sessions, and the rest of the course doesn't build much on it, so don't get bogged down in the details.</p> Natural gradient and stochastic variational inference <ul> <li>Natural gradient tutorial (coming soon!)</li> <li><a href="http://www.jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf">Hoffman, Blei, Wang, and Paisley, 2013. Stochastic variational inference</a></li> <li>Note: this material is included because it's used by Hensman et al., and also because natural gradient and SVI are just good things to know about.</li> </ul> Sparse GPs <ul> <li><a href="http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf">Titsias, 2009. Variational learning of inducing variables in sparse Gaussian processes</a></li> <li><a href="http://papers.nips.cc/paper/6477-understanding-probabilistic-sparse-gaussian-process-approximations">Bauer, van der Wilk, and Ghahramani, 2016. Understanding probabilistic sparse Gaussian process approximations.</a></li> <li><a href="https://arxiv.org/abs/1309.6835">Hensman, Fusi, and Lawrence, 2013. Gaussian processes for big data.</a></li> </ul> Variational inference and generalization <ul> <li><a href="http://www.jmlr.org/papers/volume3/seeger02a/seeger02a.pdf">Seeger, 2002. PAC-Bayesian generalization error bounds for Gaussian process classification</a> (Section 3 can be skimmed)</li> </ul> </td> </tr> <tr> <td> 10/20 </td> <td> Exploration I: Active Learning and Bandits </td> <td> Active Learning <ul> <li><a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.1992.4.4.590">MacKay, 1992. Information-based objective functions for active data selection.</a></li> <li><a href="https://arxiv.org/abs/1704.03003">Graves et al., 2017. Automated curriculum learning for neural networks.</a></li> </ul> Bandits <ul> <li><a href="http://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/Auer+al-UCB.pdf">Auer et al., 2002. Finite-time analysis of the multiarmed bandit problem.</a></li> <li><a href="http://papersdb.cs.ualberta.ca/~papersdb/uploaded_files/633/paper_ecml06.pdf">Kocsis and Szepesvari, 2006. Bandit based Monte-Carlo planning.</a></li> <li><a href="https://arxiv.org/abs/1707.02038">Russo et al., 2017. A tutorial on Thompson sampling.</a></li> </ul> </td> </tr> <tr> <td> 10/27 </td> <td> Exploration II: Bayesian Optimization </td> <td> Bayesian optimization <ul> <li><a href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-l">Snoek, Larochelle, and Adams, 2012. Practical Bayesian optimization of machine learning algorithms.</a></li> <li><a href="https://arxiv.org/abs/0912.3995">Srinivas et al., 2010. Gaussian process optimization in the bandit setting: no regret and experimental design.</a></li> <li><a href="https://arxiv.org/abs/1502.05700">Snoek et al., 2015. Scalable Bayesian optimization using deep neural networks.</a></li> </ul> Exploiting structure <ul> <li><a href="http://papers.nips.cc/paper/5086-multi-task-bayesian-optimization">Swersky, Snoek, and Adams, 2013. Multi-task Bayesian optimization</a></li> <li><a href="https://arxiv.org/abs/1406.3896">Swersky, Snoek, and Adams, 2014. Freeze-thaw Bayesian optimization</a></li> <li><a href="http://www.cs.toronto.edu/~rgrosse/aistats2017-additive.pdf">Gardner et al., 2017. Discovering and exploiting additive structure for Bayesian optimization.</a></li> </ul> </td> </tr> <tr> <td> 11/3 </td> <td> Exploration III: Reinforcement Learning </td> <td> Model-free <ul> <li><a href="http://papers.nips.cc/paper/6500-deep-exploration-via-bootstrapped-dqn">Osband et al., 2016. Deep exploration via bootstrapped DQN.</a></li> <li>TBD</li> </ul> Model-based <ul> <li><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Deisenroth_323.pdf">Deisenroth and Rasmussen, 2011. PILCO: a model-based and data-efficient approach to policy search</a></li> <li><a href="https://arxiv.org/abs/1605.07127">Depeweg et al., 2016. Learning and policy search in stochastic dynamical systems with Bayesian neural networks</a></li> <li>TBD</li> </ul> </td> </tr> <tr> <td> 11/10 </td> <td> Adversarial Robustness </td> <td> Basic phenomena <ul> <li><a href="https://arxiv.org/abs/1412.6572">Goodfellow, Shlens, and Szegedy, 2014. Explaining and harnessing adversarial examples.</a></li> <li><a href="https://arxiv.org/abs/1701.00160">Ian Goodfellow's NIPS tutorial</a></li> <li><a href="https://arxiv.org/abs/1602.02697">Papernot et al., 2016. Practical black-box attacks against machine learning systems using adversarial examples</a></li> <li><a href="https://arxiv.org/abs/1702.06832">Kos, Fischer, and Song, 2017. Adversarial examples for generative models</a></li> </ul> Attacks and defenses <ul> <li><a href="https://arxiv.org/abs/1511.07528">Papernot et al., 2015. The limitations of deep learning in adversarial settings</a></li> <li><a href="https://arxiv.org/abs/1707.02476">Bradshaw, Matthews, and Ghahramani, 2017. Adversarial Examples, Uncertainty, and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks</a></li> <li>TBD</li> </ul> </td> </tr> <tr> <td> 11/17 </td> <td> TBD </td> <td> </td> </tr> <tr> <td> 11/24 and 12/1 </td> <td> Project Presentations </td> <td> </td> </tr> <tr> <td> 12/10 </td> <td> Project reports due </td> <td> Send by e-mail to csc2541-submit at cs dot toronto dot edu. Include "CSC2541 Final Report" in subject line. </td> </tr> </table> </div> </section> <hr id='/contents/03-software'> <section> <h2><span>Software&#58;</span></h2> <div class='lead'><p>Here is some software you may find helpful for your projects:</p> <ul> <li>deep learning frameworks <ul> <li><a href="https://www.tensorflow.org/">TensorFlow</a> (probably has the most relevant software for this course)</li> <li><a href="http://pytorch.org/">PyTorch</a></li> <li><a href="http://www.deeplearning.net/software/theano/">Theano</a></li> <li><a href="https://github.com/HIPS/autograd">Autograd</a> (lightweight autodiff framework; easier to experiment with than the other frameworks, but CPU-based)</li> </ul> </li> <li>probabilistic programming languages <ul> <li><a href="http://mc-stan.org/">Stan</a> (the most widely used PPL; uses HMC, but somewhat of a black box)</li> <li><a href="http://edwardlib.org/">Edward</a> (a PPL aimed at researchers, based on HMC and stochastic variational inference)</li> </ul> </li> <li>Gaussian processes <ul> <li><a href="https://github.com/SheffieldML/GPy">GPy</a></li> <li><a href="https://gpflow.readthedocs.io/en/latest/intro.html">GPflow</a></li> </ul> </li> <li>Bayesian optimization <ul> <li><a href="https://github.com/HIPS/Spearmint">Spearmint</a></li> <li><a href="http://www.automl.org/hpolib.html">HPOLib</a></li> </ul> </li> <li>reinforcement learning <ul> <li><a href="https://gym.openai.com/">OpenAI Gym</a></li> <li><a href="https://github.com/openai/baselines">OpenAI Baselines</a></li> </ul> </li> <li>adversarial robustness <ul> <li><a href="https://github.com/tensorflow/cleverhans">CleverHans</a></li> <li><a href="https://github.com/bethgelab/foolbox">Foolbox</a></li> </ul> </li> </ul> </div> </section> <hr> </main> <footer> <div class='row'> <div class='col-lg-12'> <p> 2017</p> </div> </div> </footer> </div> <script src='/assets/app.js'></script> </body> </html>
