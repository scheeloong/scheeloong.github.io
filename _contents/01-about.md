---
id: about 
name: About
heading: About Me
subheading: About Me&#58; 
---
<img src="images/pic.png" alt="hi" class="inline"/>

# Education 

**Name:** [Soon Chee Loong](http://scheeloong.github.io)

**Email:** cheeloong.soonatmaildotutorontodotca 

**Undergraduate:**  BASc Computer Engineering 2017

**Graduate:**  MASc Industrial Engineering 2019 (expected)

* Bayesian Deep Learning approaches to Recommendation Systems

# About

I am a first year MASc (Masters of Applied Science, research-based) student at the University of Toronto.

I am a member of the [Data Driven Decision Making Laboratory](http://d3m.mie.utoronto.ca/members/).

I am curious about our intelligent ability to learn.

I am passionate about Artificial Intelligence, especially Machine Learning, as it helps me understand how learning works.

For instance, how do we model:
* Instantaneous Learning: How do we train a model to learn from correct examples? Ideas from Mathematical Optimization, Sampling, Variational Inference
* Delayed Learning: How do we learn by rewards and punishment? Ideas from Animal Learning, Reinforcement Learning
* Inference: How do we calculate probability of events given evidences? Ideas from Probabilistic Graphical Models.
* Planning: How do we plan what to do given our available information? Ideas from Markov Decision Processes.
* Information: ideas from Information Theory, Uncertainty, bayesian posterior beliefs distribution.
* Signal Processing: How do we decide if an input is useful information or noise? Ideas from Signal Detection Theory.
* Memory: How do we remember information? ideas from Sequential Circuits in Digital Logics, to delayed processing. 
* Attention: The idea of focusing. Where to focus? When to focus? How long should we focus? Ideas from Information Retrieval
* Active Learning: Idea of deciding what to learn, since it's impossible to learn everything.
* Transfer Learning: How do we transfer the useful learnt information from one model to another? 
* Knowledge Distillation: How do we transfer the core information from a large model to a smaller model?
* Few Shot Learning: How do we learn from as few examples as possible? 
* Exploration vs Exhaustation: How do we decide if we should explore other decisions or dive into ones we already know?
* Semi-Supervised Learning: How do we learn about the population given a sample? 
* Imbalanced Learning: How do we learn about rare labels when our dataset is imbalanced? 

Machine Learning research can be broken down into 4 steps: 
* Step 1: What is the problem? What is the type of data we are working with?
* Step 2: Modelling. What model should we use? What are the potential abilities of this model? 
* Step 3: How do we feasible train the model given our data?
* Step 4: Evaluation. How do we evaluate how well this model has learnt and achieve its potential abilities of this model?

I enjoy working with applied math and code. 

* [Statement Of Purpose](./pdf/statementOfPurposeUofTMIE.pdf)

# Undergraduate 

During my undergraduate, I have done Software Engineering Internships at Google, Salesforce, Intel. 

Published a [paper](https://link.springer.com/article/10.1007/s10601-016-9238-x) under the supervision of [Professor Christopher Beck](https://www.mie.utoronto.ca/mie/faculty/beck).

Final year research group capstone project under the supervision of [Professor Raquel Urtasun](http://www.cs.toronto.edu/~urtasun/).

More information on my projects in undergraduate [here](http://www.sooncheeloong.com).
