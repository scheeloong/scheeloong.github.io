---
id: calendar
name: Calendar
heading: Calendar
subheading: Calendar&#58;
image: ""
---

|           | Optional reading                | Topic
|-----------|------------------------|---------
| **Week 1** |            | 
| Lecture: Monday, Jan 09 |            | Introduction <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Lec1-intro.pdf)
| Lecture: Thursday, Jan 12 |            | Review of probability <br/> Fundamentals of machine learning <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Lec2-fundamental.pdf)
| Tutorial |            | Review of linear algebra <br/> Introduction to TensorFlow <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut1.pdf) <br/>  [Tutorial examples](http://www.psi.toronto.edu/~jimmy/ece521/tutorial1.ipynb) as an IPython Notebook. <br/> Take a look at [here](http://jupyter.readthedocs.io/en/latest/install.html) on how to install Jupyter/IPython Notebook.
| **Week 2** |            | 
| Lecture: Monday, Jan 16 | The curse of dimensionality: *Bishop 2006, Chap. 1.4* <br/> K-NN: *Bishop 2006, Chap. 2.5.2* <br/> (free) K-NN and linear regression: *Hastie et al 2013, Chap. 2.3*  <br/> (free) Convex function and Jensen inequality: *MacKay 2003, Chap. 2.7*  <br/> (free) Gradient descent: *Goodfellow et al 2016, Chap. 4.3*      | Example: K Nearest Neighbours <br/> Optimization <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Lec3-kNNoptim.pdf)
| Lecture: Thursday, Jan 19 |   [Stochastic gradient descent, Léon Bottou](http://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf) <br/> [The momentum method: *Coursera video: Neural Networks for Machine Learning Lecture 6.3*](https://www.youtube.com/watch?v=LdkkZglLZ0Q) <br/> [Maximum likelihood for a Gaussian: *MacKay 2003, Chap. 22.1*](http://www.inference.phy.cam.ac.uk/itprnn/book.pdf)  <br/> [Maximum likelihhod estimation of a classifier: *Hastie et al 2013, Chap. 2.6.3*](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf)   <br/> [Regularization: *Goodfellow et al 2016, Chap. 7.1*](http://www.deeplearningbook.org/contents/regularization.html) <br/> [Regularization through data augmentation: *Goodfellow et al 2016, Chap. 7.4*](http://www.deeplearningbook.org/contents/regularization.html)      | Maximum likelihood estimation (MLE)]<br/> Optimization and regularization <br/>[pdf slides](http://www.mebden.com/ECE521/Lec4.pdf)
| Tutorial |            | Tricks to improve SGD  <br/> "Tuning/debugging" optimizer <br/> Multivariate Gaussian  <br/> Underfitting vs. overfitting  <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut2.pdf)
| **Week 3** |            | 
| Lecture: Monday, Jan 23 |            | Probabilistic interpretation of linear regression <br/> MLE vs. MAP <br/> Optimal regressor <br/> [pdf slides] (see the joint slide deck from Jan 26)
| Assignment 1: Wednesday, Jan 25 | Due date: Feb 7 midnight, 2017 <br/> k-NN, Gaussian process (bonus), linear regression            | [Assignment handout](http://www.psi.toronto.edu/~jimmy/ece521/a1.pdf) <br/> Download Tiny MNIST dataset [here](http://www.psi.toronto.edu/~jimmy/ece521/tinymnist.npz) <br/> [Histogram of results](http://mebden.com/ECE521/histAss1_ECE.png)
| Lecture: Thursday, Jan 26 | Regression and decision theory: *Bishop 2006, Chap. 1.5* <br/> Bias-variance trade-off: *Bishop 2006, Chap. 3.2*            | Optimal regressor <br/> Feature expansion <br/> Decision theory <br/> [joint pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Lec5and6.pdf)
| Tutorial |            | k-NN, Linear regression  <br/> Gaussian process regression <br/> Training, validation and test set  <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut3.pdf)
| **Week 4** |            | 
| Lecture: Monday, Jan 30 |            | Recap decision theory <br/> Logistic regression <br/> Neural networks <br/> [pdf slides](http://www.mebden.com/ECE521/Lec7-logistic.pdf)
| Lecture: Thursday, Feb 2 | Neural networks: *Bishop 2006, Chap. 5*, *MacKay 2003, Chap. 39-40, 44*, *Hastie et al 2013, Chap. 11* <br/>            | Neural networks <br/> Backpropagation <br/> [pdf slides](http://www.mebden.com/ECE521/Lec8-nnP.pdf)
| Tutorial |            | Logistic regression <br/>  Backpropagation examples  <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut4.pdf)
| **Week 5** |            | 
| Lecture: Monday, Feb 6 |            | Multi-class classification <br/> Learning feedforward neural networks <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Lec9-nn2.pdf)
| Lecture: Thursday, Feb 9 | [Convolutional neural networks: cs231n course slides](http://cs231n.github.io/convolutional-networks/) <br/> [Transfer learning and fine-tuning: cs231n course slides](http://cs231n.github.io/transfer-learning/)           | Bag-of-tricks for deep neural networks <br/> Types of neural networks: convolutional neural networks, recurrent neural networks <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Lec10-nn3.pdf)
| Assignment 2: Thursday, Feb 9 | Due date: Feb 27 midnight, 2017 <br/> logistic regression, neural networks            | [Assignment handout (updated Feb 18th)](http://www.psi.toronto.edu/~jimmy/ece521/a2.pdf) <br/> Download notMNIST dataset [here](http://www.psi.toronto.edu/~jimmy/ece521/notMNIST.npz) <br/> [Histogram of results](http://www.psi.toronto.edu/~jimmy/ece521/histAss2_ECE.png)
| Tutorial |            | Sample midterm review <br/> Assignment 1 post-mortem
| **Week 6** |            | 
| Lecture: Mon, Feb 13 | Bishop 9.1 and 12.1           | <a href="http://mebden.com/ECE521/Lec11.pdf">k-means clustering, dimensionality reduction</a>
| Study: Thu, Feb 16 |            | Study independently in the classroom, with instructor on hand for questions. Unstructured.
| Midterm: Thursday, Feb 16 | Time: 6:20-7:50 pm.  | [Sample midterm from 2016](http://www.psi.toronto.edu/~jimmy/ece521/ECE521_midterm_2016.pdf) <br/> [Midterm cheatsheet template](http://undergrad.engineering.utoronto.ca/wp-content/uploads/2016/06/Examination-Aid-Sheet-0215.pdf)<br/>[Histogram of results](http://mebden.com/ECE521/histMidtermECE.png)
| Tutorial |            | Midterm exam post-mortem
| **Week 7** |            | 
| Lecture: Mon, Feb 27 | Bishop 3.3<br/>  Murphy 2012: parts of chap. 5 & sec. 7.6   | <a href="http://mebden.com/ECE521/Lec12.pdf">PCA continued, Bayesian methods</a>
| Lecture: Thu, Mar 2 | Bishop 1.2.6 (Bayesian prediction), 1.3 (model selection), 2.4.2 (conjugate prior)   | [Bayesian learning continued](http://mebden.com/ECE521/Lec12.pdf)
| Tutorial |    | Examples of PCA, k-Means <br/> Bayesian predictive distribution <br/> Bayesian model comparison <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/tut7.pdf)
| **Week 8** |            | 
| Lecture: Mon, Mar 6 | Mixture of Gaussians: Bishop 9.2  <br/> EM algorithm: Bishop 9.3   | [Mixture models, EM algorithm](http://mebden.com/ECE521/Lec14-15.pdf)
| Assignment 3: Wed, Mar 8 | Due date: March 24 midnight, 2017 <br/> Unsupervised learning, probablistic models             | [Assignment handout (updated Mar 13th)](http://www.psi.toronto.edu/~jimmy/ece521/a3.pdf) <br/> Download the datasets: [data2D](http://www.psi.toronto.edu/~jimmy/ece521/data2D.npy), [data100D](http://www.psi.toronto.edu/~jimmy/ece521/data100D.npy), [tinymnist](http://www.psi.toronto.edu/~jimmy/ece521/tinymnist.npz) <br/> Download the utility function [here](http://www.psi.toronto.edu/~jimmy/ece521/utils.py)
| Lecture: Thu, Mar 9 | Naive Bayes: Hastie et al 2013, Chap. 6.6.3 <br/> Bayesian network: Bishop 8.1, 8.2   | [Mixture of Gaussians, Naive Bayes and Bayesian Networks](http://www.psi.toronto.edu/~jimmy/ece521/lec15.pdf)
| Tutorial |    | Introducing A3 <br/> Examples of Mixture of Bernoullis <br/> EM algorithm <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut7.pdf)
| **Week 9** |            | 
| Lecture: Mon, Mar 13 |Bishop 8.1 & 8.2<br/>Parts of Murphy Ch. 10<br/>Russell and Norvig 2009 (<i>AI: A Modern Approach</i>) parts of Ch. 14   | [Bayesian networks continued](http://mebden.com/ECE521/Lec16.pdf)
| Lecture: Thu, Mar 16 | Bishop 8.3, 8.4.3   | [Markov Random Fields, factor graphs](http://mebden.com/ECE521/Lec17.pdf)
| Tutorial |    | Review of graphical models <br/> Conversion between BN, MRF and FG <br/> Inference in graphical models  <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut8.pdf)
| **Week 10** |           |
| Lecture: Mon, Mar 20 | Russell & Norvig 15.1<br/>Parts of Bishop Chap. 13   | [Sequence models](http://www.psi.toronto.edu/~jimmy/ece521/Lec18-hmm.pdf) 
| Lecture: Thu, Mar 23 | Parts of Russell & Norvig 15.3<br/>Parts of Bishop Chap. 13  | [Hidden Markov Models (HMMs)](http://www.psi.toronto.edu/~jimmy/ece521/Lec19-hmm2.pdf)
| Tutorial |    | Review of Markov models <br/> Examples of inference in graphical models  <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut9.pdf)
| **Week 11** |       |
| Lecture: Mon, Mar 27 | Murphy 17.4<br/>End of Russell & Norvig 15.2<br/>Bishop 13.2.5    | [HMM inference/learning](http://www.mebden.com/ECE521/Lecture20HLI.pdf)
| Assignment 4: Mon, Mar 27 | Due date: April 9th midnight, 2017 <br/> Graphical models, sum-product algorithm         | [Assignment handout (updated)](http://www.psi.toronto.edu/~jimmy/ece521/a4.pdf)
| Lecture: Thu, Mar 30 | Parts of MacKay Chapter 16 and Sections 26.1-26.2<br/>Bishop 8.4.4  <br/> [Kschischang, Frey and Loeliger: Factor Graphs and the Sum-Product Algorithm](www.psi.toronto.edu/~frey/papers/fgspa.ps) Section 2 <br/> [Frey: Extending Factor Graphs so as to Unify Directed and Undirected Graphical Models](https://arxiv.org/pdf/1212.2486) Section 2  |  [Message-passing algorithms (updated notation)](http://www.psi.toronto.edu/~jimmy/ece521/Lec21-mp.pdf) 
| Tutorial |    | Forward-backward algorithm <br/> The sum-product algorithm  <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut10.pdf)
| **Week 12** |       |
| Lecture: Mon, Apr 3 | Murphy 17.4.4 & 20.2<br/>Bishop 8.4.5 & 13.2.5<br/>MacKay 26.3  | [Max-sum algorithm](http://www.mebden.com/ECE521/Lec22.pdf)
| Lecture: Thu, Apr 6 |  MacKay Chapters 16 and 26<br/> Bishop 8.4.7<br/>LBP: MacKay 26.4, Bishop 8.4.7  | [Junction-tree algorithm, Loopy belief propagation](http://www.mebden.com/ECE521/Lec23.pdf)
| Tutorial |    | Review  <br/> [pdf slides](http://www.psi.toronto.edu/~jimmy/ece521/Tut11.pdf)
| **Week 13** |       |
| Lecture: Mon, Apr 10 | [The first section of Murphy 19.6](https://www.cs.ubc.ca/~murphyk/MLbook/pml-print3-ch19.pdf)   | [Supervised Learning using Graphical Models <br/> Discriminative Approach <br/> Conditional Random Fields (CRFs) <br/> Combining Deep Learning with Graphical Models](http://www.psi.toronto.edu/~jimmy/ece521/Lec24.pdf) 
| Lecture: Thu, Apr 13 |  All the above  | [Course concepts](http://www.psi.toronto.edu/~jimmy/ece521/Lec25-review.pdf), the 2013 midterm, and finishing our junction-tree algorithm example
| **Exam**|      |
| Thu, Apr 20 |       | For study practice: [2013 midterm](http://mebden.com/ECE521/midterm2013.pdf)
